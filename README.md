# ğŸ–¼ï¸ VisionSpeak  
### Attention-Based Image Captioning System (CNN + LSTM + Additive Attention)

<p align="center">
  <img src="glimpse.png" width="100%">
</p>

---

## ğŸ“Œ 1. Project Overview

**VisionSpeak** is a deep learningâ€“based image captioning system that generates natural language descriptions for images.

The system learns to:

- Extract spatial visual features from an image  
- Focus on different regions of the image while generating each word  
- Produce coherent captions using sequence modeling  
- Provide interpretable attention heatmaps  

### ğŸ¯ Core Objective

> Given an image â†’ Generate a grammatically meaningful caption describing the scene.

This project implements a classical **Encoderâ€“Decoder architecture with Attention**.

---

## ğŸ§  2. Key Concepts (Important Terms)

### ğŸ”¹ Convolutional Neural Network (CNN)

A **CNN** is a deep learning architecture designed for image understanding.

It extracts hierarchical spatial features using convolutional filters.

In this project:

- A pretrained CNN is used to extract image features.
- Each image is converted into a spatial feature map of shape: ```(49, 2048)```

---

This represents:

- 7 Ã— 7 spatial grid  
- 2048 feature dimensions per region  

These 49 feature vectors allow spatial attention.

---

### ğŸ”¹ Long Short-Term Memory (LSTM)

An **LSTM** is a type of Recurrent Neural Network (RNN) used for modeling sequential data.

It maintains:

- Hidden state (short-term memory)  
- Cell state (long-term memory)  

In this project:

- The LSTM generates captions word-by-word.
- At each timestep, it predicts the next word conditioned on:
  - Previous words
  - Visual context

---

### ğŸ”¹ Attention Mechanism

Attention allows the model to focus on different parts of the image while generating each word.

Instead of compressing the image into a single vector, attention:

- Computes weights over all 49 spatial regions  
- Produces a weighted combination (context vector)  
- Uses this context to generate the next word  

This makes the model:

- More accurate  
- More interpretable  

---

## ğŸ“ 3. Problem Definition

Image captioning is a multimodal task combining:

- Computer Vision (understanding image content)  
- Natural Language Processing (generating text)  

Given image \( I \), generate caption:

\[
S = (w_1, w_2, ..., w_T)
\]

The probability model:

\[
P(S|I) = \prod_{t=1}^{T} P(w_t | w_{1:t-1}, I)
\]

The system learns to predict the next word conditioned on:

- Previous words  
- Visual features  

---

## ğŸ—ï¸ 4. Architecture

### ğŸ”¹ 4.1 Encoder

Instead of training a CNN from scratch, **pre-extracted CNN features** are used.

Each image representation:```Shape: (49, 2048)```








